# Seleciona explicitamente o provider
spring.ai.model.chat=ollama
spring.ai.model.embedding=ollama

# Ollama nativo (mesmo host/porta do teu docker)
spring.ai.ollama.base-url=http://localhost:11434

# Modelos que já tens no daemon
spring.ai.ollama.chat.options.model=lfm2-350m:latest
spring.ai.ollama.embedding.options.model=nomic-embed-text
